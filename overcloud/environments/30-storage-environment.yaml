parameter_defaults:
  # Whether to enable iscsi backend for Cinder.
  CinderEnableIscsiBackend: false

  # Whether to enable NFS backend for Cinder.
  CinderEnableNfsBackend: false

  # Whether to enable rbd (Ceph) backend for Cinder.
  CinderEnableRbdBackend: true

  # Cinder Backup backend can be either 'ceph', 'swift' or 'nfs'.
  CinderBackupBackend: ceph

  # Glance backend can be either 'rbd' (Ceph), 'swift' or 'file'.
  GlanceBackend: rbd

  # Here the Ceph Storage node config under Linux HCI
  # NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
  # sda      8:0    0  50G  0 disk
  # ├─sda1   8:1    0   1M  0 part
  # └─sda2   8:2    0  50G  0 part /
  # sdb      8:16   0   8G  0 disk <- Optane NVME
  # sdc      8:32   0  75G  0 disk <- SSD Data
  # sdd      8:48   0  75G  0 disk <- SSD Data

  # Here the Ceph Storage node config under ESXi HCI
  # NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
  # sda       8:0    0  50G  0 disk
  # ├─sda1    8:1    0   1M  0 part
  # └─sda2    8:2    0  50G  0 part /
  # sdb       8:16   0  75G  0 disk <- SSD Data
  # sdc       8:32   0  75G  0 disk <- SSD Data
  # nvme0n1 259:0    0   8G  0 disk <- Optane NVME

  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
    dedicated_devices:
      - /dev/nvme0n1
    osd_scenario: non-collocated
    # It is recommended that the block.db size isn’t smaller than 4% of block.
    # For example, if the block size is 1TB, then block.db shouldn’t be less than 40GB.
    # In our environment we have 2x 75GB per Ceph Storage Node so we configure a 3GiB block.db
    # block_db_size is in byte
    block_db_size: 3222000000
    osd_objectstore: bluestore

  # Additiona Ceph Configs
  CephConfigOverrides:
    # Max PG Number per OSD
    mon_max_pg_per_osd: 2048
    # Increase max filelimit
    # Default is 32K
    max_open_files: 131072
    # Support all the base OpenStack roles
    rgw_keystone_accepted_roles: 'Member, _member_, admin, swiftoperator'

  CephAnsiblePlaybookVerbosity: 1

  # Required variables to run config-download manually
  # https://docs.openstack.org/project-deploy-guide/tripleo-docs/latest/deployment/ansible_config_download.html#run-ansible-playbook
  CephAnsibleEnvironmentVariables:
    ANSIBLE_HOST_KEY_CHECKING: 'False'
    ANSIBLE_PRIVATE_KEY_FILE: '/home/stack/.ssh/id_rsa'
    ANSIBLE_REMOTE_USER: 'heat-admin'

  # Default Replica Count
  # HDD CephPoolDefaultSize: 3 (and higher)
  # Flasg CephPoolDefaultSize: 2 (and higher)
  CephPoolDefaultSize: 2

  # PG per Pool based on current hardware (Ceph PG Calc)
  CephPoolDefaultPgNum: 8

  # Ceph Pools PG numbers
  CephPools:
    - name: volumes
      pg_num: 128 # assuming 30% of the datas
      application: rbd
    - name: vms
      pg_num: 128 # assuming 30% of the datas
      application: rbd
    - name: backups
      pg_num: 64 # assuming 15% of the datas
      application: rbd
    - name: images
      pg_num: 64 # assuming 20% of the datas
      application: rbd

  # Whether to enable rbd (Ceph) backend for Nova ephemeral storage.
  NovaEnableRbdBackend: true

  ExtraConfig:
    # Disable force RAW images
    # If NovaEnableRbdBackend is set to true this flag MUST be set to true (default option)
    nova::compute::force_raw_images: true
    # Enable the Cinder Backup dashboard in Horizon
    horizon::cinder_options: { enable_backup: true }
